Possible experiments for 6.867 project:

- Test out reward shaping effects on learning. Reward shaping is defined in Ng, Harada & Russell (1999)
- Try out different ways of generating training encounters. Try out different behaviors for the intruder (Markov chain, random, single stretch of maneuvers). A Markov chain approach would consist of issuing actions depending on which state we're in. We will make 5 states (NOOP, LEFT, RIGHT, UP, DOWN). At each timestep, the Markov Chain will emit an action corresponding to the state it's in. We will need to define reasonable transition probabilities between states; these will change the format of the encounters generated. 
- Evaluate the effects of changing the ratio of encounters with NMAC vs no NMAC. 
- Can we try changing the training data to transition sampling?
- The magnitude of the state-value function is very different at different points in the state space. When we run gradient descent, this has a big impact in what updates matter most and which matter less. Updates that are the same percent change in areas of high value will have a larger impact than updates with lower magnitude state-action value functions.
- What areas of the state space are unstable? Which areas are very likely to change in the next gradient descent? We can look at what areas of the state-space have very little difference between the best action and the next-best action. This might tell us what is causing the policy to select which actions and why.


Things to do before experiment:
[x] Implement 2D coordinates.
[x] Implement only 3 actions.
[x] Use stable-baselines.
[x] Policy plot.
[x] Q-value plot.
[x] Validation encounters.
[x] Metrics collection.


Saving Training Data:
[x] Create new directory when training a model and save:
    1.[x] Hyperparameters.
    2.[x] Reward structure.
    3.[x] Training logs (tensorboard). (doesn't work)
    4.[ ] Use baselines plotting utils.
[ ] When running validation on a model, use the same training directory to save:
    1.[ ] Number and seed of validation set.
    2.[ ] Metrics collected from validation.
    3.[ ] Plotting data from validation sets.
[ ] Set up experiments:
    1. Create sets of hyperparameters and run training on each setting. Repeat same setting for 3 different seeds. Label experiment directory by experiment run.
    2. Right after training, run validation for each trained model.
    3. Aggregate the metrics and training data by experiment setting.


